chisqtestGC(~seat,data=m111survey,p=c(1/3,1/3,1/3),graph = TRUE)
chisqtestGC(~seat,data=m111survey,p=c(1/3,1/3,1/3),graph = TRUE,simulate.p.value = TRUE)
ChisqSimSlow(~weather+crowd.behavior,data=ledgejump,effects="fixed")
chisqtestGC()
chisqtestGC
?rmultinom
rmultinom(1,(sum(c(12,15,16)),prob=(1/3,1/3,1/3)))
rmultinom(1,size=(sum(c(12,15,16)),prob=c(1/3,1/3,1/3)))
rmultinom(1,size=(sum(12,15,16),prob=c(1/3,1/3,1/3)))
rmultinom(1,size=sum(12,15,16),prob=c(1/3,1/3,1/3)))
rmultinom(1,size=sum(12,15,16),prob=c(1/3,1/3,1/3))
rmultinom(10, size = 12, prob = c(0.1,0.2,0.8))
install.packages(c("alphahull", "babynames", "curl", "effects", "faraway", "gdtools", "gtrendsR", "Hmisc", "MBESS", "multcomp", "NLP", "R.methodsS3", "R.oo", "RefManageR", "ResourceSelection", "rgdal", "rjags", "rms", "shiny", "stochvol", "texreg"))
install.packages(c("ade4", "analogue", "bigrquery", "biomod2", "cocorresp", "coenocliner", "deldir", "deSolve", "evaluate", "FactoMineR", "formatR", "ggdendro", "ggplot2", "git2r", "glcm", "glmnet", "gridExtra", "gtable", "gtrendsR", "htmlwidgets", "ks", "leaflet", "Matrix", "mboost", "MCMCpack", "mgcv", "mnormt", "neotoma", "nlme", "OpenMx", "openssl", "pbapply", "R.matlab", "RcppEigen", "RCurl", "ReporteRs", "rgdal", "rockchalk", "rsvg", "scales", "semTools", "sjPlot", "spatstat", "vcdExtra", "vegan", "XML"))
install.packages(c("BiodiversityR", "car", "circlize", "commonmark", "effects", "gplots", "kernlab", "packrat", "polyclip", "R.matlab", "Rcpp", "readxl", "rgdal", "shiny", "sjPlot", "treemap"))
install_github("juliasilge/janeaustenr")
library(janeaustenr)
library(devtools)
install_github("juliasilge/janeaustenr")
library(janeaustenr)
library(dplyr)
library(stringr)
library(syuzhet)
library(ggplot2)
library(viridis)
data(sensesensibility)
data(prideprejudice)
data(mansfieldpark)
data(emma)
data(northangerabbey)
data(persuasion)
str(persuasion)
head(persuasion)
head(persuasion)[1:90]
head(persuasion)[100:190]
head(persuasion)[100:290]
persuasion
library(devtools)
install_github("MangoTheCat/tidyshiny")
library(tidyshiny)
testData <- airquality
tidyData()
tidyr::gather(data = testData, key = Month, value = Ozone, na.rm = FALSE)
testData <- airquality
tidyData()
devtools::install_github("homerhanumat/shinyCustom")
devtools::install_github("homerhanumat/addinplots")
addinplots:::barchartAddin()
addinplots:::barchartAddin()
addinplots:::xyplotAddin()
addinplots:::xyplotAddin(testData)
addinplots:::xyplotAddin()
data("ChickWeight")
data("ChickWeight")
ChickWeight
addinplots:::xyplotAddin()
addinplots:::barchartAddin()
addinplots:::histogramAddin()
devtools::install_github("BAAQMD/copydat")
addinplots:::histogramAddin()
addinexamples:::subsetAddin()
long<-c(1,10,10)
long<-c(1,10,1)
long<-c(1:10,1)
lat<-c(1:10,1)
library(rgdal)
outline<-cbind(long,lat)
foutline <-fortify(outline)
library(ggplot2)
foutline <-fortify(outline)
outline<-as.data.frame(cbind(long,lat))
foutline <-fortify(outline)
depth<-rnorm(121,mean=2,sd=0.5)
fdepth<-data.frame(depth)
ggplot(foutline, aes(x = long, y = lat)) +
geom_path() +
geom_point(data = fdepth, aes(x = os_x, y = os_y, colour = depth)) +
coord_fixed() + ylab("Northing") + xlab("Easting") +
scale_color_viridis()
ggplot(foutline, aes(x = long, y = lat)) +
geom_path() +
geom_point(data = fdepth, aes(x = os_x, y = os_y, colour = depth)) +
coord_fixed() + ylab("Northing") + xlab("Easting")
View(foutline)
install.packages(c("BiodiversityR", "caret", "cluster", "curl", "devtools", "EnvStats", "forecast", "GlobalOptions", "HH", "iNEXT", "janeaustenr", "ks", "lava", "lme4", "lubridate", "Matrix", "MCMCpack", "miniCRAN", "msir", "nlme", "pbapply", "penalized", "pipeR", "polyclip", "R.utils", "rasterVis", "Rcmdr", "RcmdrMisc", "RcppArmadillo", "RefManageR", "rgdal", "rioja", "rms", "sde", "seacarb", "sem", "sjmisc", "sp", "spdep", "survival", "testthat", "vegan", "Zelig"))
install.packages("rticles", type = "source")
install.packages("rmdformats")
devtools::install_github("rstudio/rmarkdown")
set.seed(42)
x1 <- seq(1,10,0.3)
w = .6067;
a0 = 1.6345;
a1 = -.6235;
b1 = -1.3501;
a2 = -1.1622;
b2 = -.9443;
x2 = a0 + a1*cos(x1*w) + b1*sin(x1*w) + a2*cos(2*x1*w) +
b2*sin(2*x1*w) + rnorm(length(x1),0,3/4)
x <- scale(cbind(x1,x2))
alim <- extendrange(x, f=0.1)
alim_ <- range(x)
View(x)
set.seed(42)
x1 <- seq(1,100,0.3)
w = .6067;
a0 = 1.6345;
a1 = -.6235;
b1 = -1.3501;
a2 = -1.1622;
b2 = -.9443;
x2 = a0 + a1*cos(x1*w) + b1*sin(x1*w) + a2*cos(2*x1*w) +
b2*sin(2*x1*w) + rnorm(length(x1),0,3/4)
x <- scale(cbind(x1,x2))
alim <- extendrange(x, f=0.1)
alim_ <- range(x)
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Initialize"), bty="n")
svdx <- svd(x)
clip(alim_[1],alim_[2],alim_[1],alim_[2])
with(svdx, abline(a=0, b=v[2,1]/v[1,1]))
z1 <- with(svdx, x%*%v[,1]%*%t(v[,1]))
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
lam <- with(svdx, as.numeric(u[,1]*d[1]))
for(itr in 1:3) {
#### step (a) of iterative algorithm ####
## compute scatterplot smoother in either dimension
## increase 'df' to make the curve more flexible
fit1 <- smooth.spline(x=lam, y=x[,1], df=4)
fit2 <- smooth.spline(x=lam, y=x[,2], df=4)
## plot data and the principal curve for a sequence of lambdas
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Step (a)"), bty="n")
seq_lam <- seq(min(lam),max(lam),length.out=100)
lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
## show points along curve corresponding
## to original lambdas
z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
#### step (b) of iterative algorithm ####
## recompute lambdas
euc_dist <- function(l, x, f1, f2)
sum((c(predict(f1, l)$y, predict(f2, l)$y) - x)^2)
lam <- apply(x,1,function(x0) optimize(euc_dist,
interval=extendrange(lam, f=0.50),
x=x0, f1=fit1, f2=fit2)$minimum)
## show projections associated with recomputed lambdas
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Step (b)"), bty="n")
seq_lam <- seq(min(lam),max(lam),length.out=100)
lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
}
View(z1)
plot(z1)
for(itr in 1:3) {
#### step (a) of iterative algorithm ####
## compute scatterplot smoother in either dimension
## increase 'df' to make the curve more flexible
fit1 <- smooth.spline(x=lam, y=x[,1], df=2)
fit2 <- smooth.spline(x=lam, y=x[,2], df=2)
## plot data and the principal curve for a sequence of lambdas
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Step (a)"), bty="n")
seq_lam <- seq(min(lam),max(lam),length.out=100)
lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
## show points along curve corresponding
## to original lambdas
z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
#### step (b) of iterative algorithm ####
## recompute lambdas
euc_dist <- function(l, x, f1, f2)
sum((c(predict(f1, l)$y, predict(f2, l)$y) - x)^2)
lam <- apply(x,1,function(x0) optimize(euc_dist,
interval=extendrange(lam, f=0.50),
x=x0, f1=fit1, f2=fit2)$minimum)
## show projections associated with recomputed lambdas
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Step (b)"), bty="n")
seq_lam <- seq(min(lam),max(lam),length.out=100)
lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
}
for(itr in 1:3) {
#### step (a) of iterative algorithm ####
## compute scatterplot smoother in either dimension
## increase 'df' to make the curve more flexible
fit1 <- smooth.spline(x=lam, y=x[,1], df=7)
fit2 <- smooth.spline(x=lam, y=x[,2], df=7)
## plot data and the principal curve for a sequence of lambdas
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Step (a)"), bty="n")
seq_lam <- seq(min(lam),max(lam),length.out=100)
lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
## show points along curve corresponding
## to original lambdas
z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
#### step (b) of iterative algorithm ####
## recompute lambdas
euc_dist <- function(l, x, f1, f2)
sum((c(predict(f1, l)$y, predict(f2, l)$y) - x)^2)
lam <- apply(x,1,function(x0) optimize(euc_dist,
interval=extendrange(lam, f=0.50),
x=x0, f1=fit1, f2=fit2)$minimum)
## show projections associated with recomputed lambdas
plot(x[,1], x[,2], bty='n',
xlab=expression(x[1]),
ylab=expression(x[2]),
xlim=alim, ylim=alim)
legend("topleft", legend=c("Step (b)"), bty="n")
seq_lam <- seq(min(lam),max(lam),length.out=100)
lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
segments(x0=x[,1],y0=x[,2],
x1=z1[,1],y1=z1[,2])
}
install.packages(c("MissingDataGUI", "RandomFields", "rgdal"))
install.packages("bigmemory")
scat <- function(y, x, cols=50, rows=20, pch="*") {
#make an ASCII scatterplot on a rows X cols grid
#pch is the ASCII character plotted
#check arguments
y <- as.numeric(y)
if(missing(x)) x <- 1:length(y)
else x <- as.numeric(x)
if(length(y) != length(x))
stop("lengths of y and x differ")
rows <- as.numeric(rows)
cols <- as.numeric(cols)
if(rows < 1 || cols < 1)
stop("rows and cols must be > 1")
if(nchar(pch)!=1)
stop("pch must be exactly one character")
#map the y and x values to rows and cols
#FIXME values in y or x could be NA or NaN
#FIXME division by zero when max(y)-min(y) == 0
#FIXME any better way to do this?
ymap <- floor((y-min(y))/(max(y)-min(y))*(rows-1))
xmap <- floor((x-min(x))/(max(x)-min(x))*(cols-1))
#sort the mapped values so that the are drawn in
#left-to-right top-to-bottom order, because thats
#how they will be printed, unique because we can
#only print one character in a cell
bitmap <- unique(cbind(ymap,xmap)[order(-ymap, xmap),])
#initialize row and col positions
#last plotted character row and column
row <- rows - 1
col <- 0
cat(" ", rep("_", cols+4), "\n|  ", sep="")
cat(rep(" ", cols), "  |\n|  ", sep="")
for(bit in 1:nrow(bitmap)) {
while(bitmap[bit,1] != row) {
if(cols-col > 0)
cat(rep(" ", cols-col), sep="")
cat("  |\n|  ")
row <- row - 1
col <- 0
}
if(bitmap[bit,2]-col > 0)
cat(rep(" ", bitmap[bit,2]-col), sep="")
cat(pch)
col <- bitmap[bit, 2] + nchar(pch)
}
if(cols-col > 0)
cat(rep(" ", cols-col), sep="")
cat("  |\n|", rep("_", cols+4), "|\n", sep="")
invisible(bitmap)
}
scat(1:10,1:10)
scat(x[1],x[1])
scat(x[1,],x[1,])
scat(x[,1],x[,1])
scat(x[,1],x[,1],cols = 100,rows = 100)
View(x)
scat(x$x1,x$x1,cols = 100,rows = 100)
scat(x$x1,x$x2,cols = 100,rows = 100)
scat(x[,1],x$x2,cols = 100,rows = 100)
scat(x[,1],x[,2],cols = 100,rows = 100)
scat(x[,1],x[,2],cols = 10,rows = 10)
scat(x[,1],x[,2],cols = 100,rows = 10)
scat(x[,1],x[,2],cols = length(x),rows = length(x))
dims(x)
dim(x)
dim(x)$1
dim(x)[1]
scat(x[,1],x[,2],cols = dim(x)[1],rows = dim(x)[1])
zz<-scat(x[,1],x[,2],cols = dim(x)[1],rows = dim(x)[1])
View(zz)
plot(zz[1],zz[2])
plot(zz[,1],zz[,2])
scat(x[,1],x[,2],cols = 20,rows = 10)
install.packages("googleformr")
install.packages(c("formatR", "highr", "iNEXT", "jsonlite", "knitr", "MissingDataGUI", "rgdal", "spatstat"))
rnorm(10,3,2)
mean(rnorm(10,3,2))
library(vegan)
setwd("~/Research/Studio/bigTrain")
GDAload <- read.csv("GDAsh.csv",header=TRUE, stringsAsFactors=FALSE)
View(GDAload)
Species <- GDAload[,21:98]
Vars <- GDAload[,6:15]
View(Vars)
# impute from NA ----
Vars[1224,1]<-12.0
dat.sub.half <-Species[,colSums(Species[,1:78]>0)>=nrow(Species)/2]
dat.sub.quar <-Species[,colSums(Species[,1:78]>0)>=nrow(Species)/4]
# Scale and Center (pre-processing) ----
pp_dsh <- preProcess(dat.sub.half, method = c("center", "scale", "YeoJohnson"))
library(caret)
library(gbm)
library(lattice)
pp_dsh <- preProcess(dat.sub.half, method = c("center", "scale", "YeoJohnson"))
pp_dsh.trans <- predict(pp_dsh, newdata = dat.sub.half)
pp_dsq <- preProcess(dat.sub.quar, method = c("center", "scale", "YeoJohnson"))
View(pp_dsh.trans)
# Plots of Training Perf testin
View(Species)
View(pp_dsh.trans)
View(dat.sub.half)
View(pp_dsh.trans)
setwd("~/Research/MAP")
# Chunk 1: knitr_init
library(knitr)
library(rmdformats)
## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
cache=FALSE,
prompt=FALSE,
tidy=FALSE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
# Chunk 2: load_libs
library(plyr)
library(dplyr)
library(tidyr)
library(lattice)
library(sjPlot)
# Chunk 3: load_func
source("R/howarthCI.R") # compute Howarth CI's
source("R/computeSPG.R") # compute Howarth CI's
source("R/threshCounts.R") # threshold a dataframe
# Chunk 4: read_dat
dataTable <- read.table("data/MAP_dat.tsv",header=T)
write.table(dataTable,file="data/rawData.tsv",sep="\t",col.names=T,row.names = FALSE)
# Chunk 5: agg_dat
dt <-gather(dataTable,key="property",value="frac_grams",-name,-depth,-sam,-dw) %>%
mutate(property=factor(property, levels = unique(property))) %>%
group_by(name,depth,dw,property) %>% summarise(Total=sum(frac_grams)) %>%
spread(property,Total) %>% ungroup() %>% mutate(sample_Size = rowSums(.[5:38])) # aggregate by sam and depth
kable(dt)
# Chunk 6: ass_char
occurences.bottom <- gather(dataTable,key="property",value="frac_grams",-name,-depth,-sam,-dw,-frac) %>%
group_by(property) %>% filter(depth >=696) %>% summarise(Total=sum(frac_grams)) %>%
.[order(-.$Total),]  %>% mutate(Prevelance=round((.$Total / sum(.$Total)*100),2)) # lower part of core distribution
occurences.top <- gather(dataTable,key="property",value="frac_grams",-name,-depth,-sam,-dw,-frac) %>%
group_by(property) %>% filter(depth <=670) %>% summarise(Total=sum(frac_grams)) %>%
.[order(-.$Total),]  %>% mutate(Prevelance=round((.$Total / sum(.$Total)*100),2)) # upper part of core distribution
occurences.total <- gather(dataTable,key="property",value="frac_grams",-name,-depth,-sam,-dw,-frac) %>%
group_by(property) %>% summarise(Total=sum(frac_grams)) %>%
.[order(-.$Total),]  %>% mutate(Prevelance=round((.$Total / sum(.$Total)*100),2)) # entire core distribution
common.species.80 <- dt %>% select(Brsp:Pyrt) %>% .[,colSums(.[,]>=1)>=nrow(.)*0.8] %>%
mutate(A.I.Ratio = round(rowSums(.)/rowSums(dt[,5:38]),2)) # by prevelance
# Chunk 7: dat_prop
species.Proportions <- dt %>% select(Brsp:Pyrt) %>%  `/` (rowSums(.)) %>% `/` (0.01) %>% round(2)
CI.LowerBounds <- dt %>% select(Brsp:Pyrt) %>% as.matrix() %>%
betLB(n=.,N=rowSums(.),alpha=0.025) %>% round(2)
CI.UpperBounds <- dt %>% select(Brsp:Pyrt) %>% as.matrix() %>%
betUB(n=.,N=rowSums(.),alpha=0.025) %>% round(2) # PO Howarth CI's
# Chunk 8: dat_conc
dt$samConc <- dt %$% sPg(sample_Size,dw,frac) %>%  round(2)
species.Concentrations <- species.Proportions %>% `/` (100) %>% `*` (dt$samConc) %>% round(2) # species conc
dt$samConc.LL <- CI.LowerBounds %>% `/` (100) %>% `*` (dt$samConc) %>% rowSums(.) %>% round(2) # lower bounds
dt$samConc.UL <- CI.UpperBounds %>% `/` (100) %>% `*` (dt$samConc) %>% rowSums(.) %>% round(2) # upper bounds
View(dataTable)
View(dt)
View(occurences.bottom)
View(dataTable)
View(species.Proportions)
View(species.Concentrations)
View(CI.UpperBounds)
View(CI.LowerBounds)
View(species.Proportions)
setwd("~/Research/runTrack")
library(XML)
filenames.all <-list.files()
setwd("~/Research/runTrack")
setwd("~/Research/runTrack/data-processed")
filenames.all <-list.files()
beg=1
end=as.numeric(length(filenames.all))
red<-as.character(strsplit(filenames.all,".xml"))
# Main Loop
for (i in beg:end) {
reader<-paste0(filenames.all[i])
writer1<-paste0("Tree",filenames.all[i])
writer2<-paste0("Node",filenames.all[i])
writer3<-paste0("List",i)
write.final<-paste0(red[i])
# parse and root
tempTree <-xmlTreeParse(reader)
xmlNode <- xmlRoot(tempTree)
assign(writer1,tempTree)
assign(writer2,xmlNode)
# extract lists
tempList<-xmlSApply(xmlNode, function(x) xmlSApply(x, xmlValue))
assign(writer3,tempList)
# cut awy from edges of matrix due to improper filling
k=6
len=length(tempList)-6
Run=matrix(data=NA, nrow=len, ncol=k)
for(j in 1:k){
for(f in 1:len){
Run[f,j] = tempList[4+f]$TrackPoints[1+j]
}
}
assign(write.final,Run)
Run <-matrix(as.numeric(Run),nrow=len,ncol=k)
colnames(Run) <- c("Lat","Lon","Altitude","Velocity","HR","TimeInt")
# transfrom to data.frame and convert
Run.df<-as.data.frame(Run)
Run.df$timeTotal <- cumsum(Run.df$TimeInt)
Run.df$speedMS <- round(Run.df$Velocity/(3600)*(1000),2)
Run.df$distanceInt <- round(Run.df$speedMS*Run.df$TimeInt,2)
Run.df$distance <-cumsum(Run.df$distanceInt)
Run.df$date <-as.Date(red[i])
assign(write.final,Run.df)
}
# clean junk away
rm(list = ls(pattern=".xml*"))
rm(list = ls(pattern=".df*"))
rm(list = ls(pattern="Run"))
rm(list = ls(pattern = "^List"))
rm(list = ls(pattern = "^List"))
dfs <-Filter(function(x) is(x, "data.frame"), mget(ls()))
all.Data <- do.call(rbind,dfs)
View(all.Data)
View(`2015-7-26`)
library(plyr)
all.Data$dates<-as.factor(all.Data$date)
daply(all.Data[,c(2)],.(date),colwise(mean))
daply(all.Data,.(date),test=mean(HR))
summary<-ddply(all.Data, .(dates),summarize,
mean = round(mean(HR), 2),
sd = round(sd(HR), 2),cv=sd/mean)
summary$CV<-sd/mean
all.Data$MetresRun <-cumsum(all.Data$distanceInt)
View(summary)
View(all.Data)
# order HR by velocity (rank HR based on Velocities)
HR.vel<-all.Data$HR[order(all.Data$HR,all.Data$Velocity,decreasing = TRUE)]
VEL.vel<-all.Data$Velocity[order(all.Data$HR,all.Data$Velocity,decreasing = TRUE)]
Peak.HR.vel<-mean(HR.vel[1:ind])
Peak.VEL.vel<-mean(VEL.vel[1:ind])
speeds<-c(0,Peak.VEL,Peak.VEL.vel)
HRs <-c(72,Peak.HR,Peak.HR.vel)
plot(speeds,HRs,cex=0.9,pch=16,main="HR-Running Speed Index",col=c("blue","orange","red"),
xlab = "Speed (km.h-1)", ylab = "HR (bpm)",xlim = c(0,15),
cex.main=0.9,cex.lab=0.8,cex.axis=0.8)
fit<-lm(HRs~speeds)
abline(fit,col="grey",lwd=2)
running.index <-function(HR,speed,rest){
out <-speed/((HR-rest)/(185-rest))
}
# Imputation
is.na(all.Data$HR) <- !all.Data$HR # first get all zeros in HR to NA
library(mice)
imp <-mice(all.Data,m=5) # full imputation: all variables
imp
imp_tot <- complete(imp, inc = TRUE)
mat <-matrix(0,ncol=11,nrow=11)
mat[5,4]<-1
mat[5,7]<-1
mat[5,10]<-1
imp2 <-mice(all.Data,m=5,predictorMatrix = mat) # partial imputation
imp2_tot <- complete(imp2, inc = TRUE)
par(mfrow=c(3,1))
plot(all.Data$MetresRun,all.Data$HR,pch=16,cex=0.3,type="p",
xlab="Distance Run (m)", ylab="HR", cex.lab=0.7,cex.axis=0.7,font.lab=3,font.axis=2)
plot(imp_tot$MetresRun,imp_tot$HR,pch=16,cex=0.3,type="p",
xlab="Distance Run (m)", ylab="HR", cex.lab=0.7,cex.axis=0.7,font.lab=3,font.axis=2)
plot(imp2_tot$MetresRun,imp2_tot$HR,pch=16,cex=0.3,type="p",
xlab="Distance Run (m)", ylab="HR", cex.lab=0.7,cex.axis=0.7,font.lab=3,font.axis=2)
